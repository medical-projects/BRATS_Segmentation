{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras \n",
    "\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, UpSampling2D, \\\n",
    "            Flatten, Dense, Conv2D, MaxPooling2D, ZeroPadding2D,\\\n",
    "            AveragePooling2D\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.applications import ResNet50, densenet\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_class:\n",
    "    def __init__(self, pat_n, image, roi, n_serial):\n",
    "        \n",
    "        self.name = pat_n\n",
    "        self.image = image\n",
    "        self.roi = roi\n",
    "        self.mal = False\n",
    "        self.n_serial = n_serial\n",
    "        \n",
    "        if np.sum(roi) > 0:\n",
    "            self.mal = True\n",
    "        \n",
    "    def print_roi(self, cmap = 'plasma', linewidths = 0.5,  **args):\n",
    "        plt.contour(self.roi, cmap = cmap, linewidths = linewidths, **args)\n",
    "            \n",
    "    def print_image(self, cmap = 'gray', **args):\n",
    "        plt.imshow(self.image, cmap = 'gray', **args)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(poss, negs, admitted_pos, num_test = 50 ):\n",
    "    negs.pop('Brats18_CBICA_AWI_1')\n",
    "    poss.pop('Brats18_CBICA_AWI_1')\n",
    "    \n",
    "    poss_shuffled = list(poss.keys())\n",
    "    random.shuffle(poss_shuffled)\n",
    "    \n",
    "    test_pts = poss_shuffled[ :num_test]\n",
    "    train_pts = poss_shuffled[num_test:]\n",
    "    \n",
    "    files_pos_train = reduce(lambda x,y:x+y, [poss[x] for x in train_pts])\n",
    "    files_neg_train = reduce(lambda x,y:x+y, [negs[x] for x in train_pts])\n",
    "\n",
    "    files_pos_train = [x for x in files_pos_train if x in admitted_pos]\n",
    "    random.shuffle(files_neg_train)\n",
    "    \n",
    "    files_train = files_pos_train + files_neg_train[:len(files_pos_train)]\n",
    "    \n",
    "    files_pos_test = reduce(lambda x,y:x+y, [poss[x] for x in test_pts])\n",
    "    files_neg_test = reduce(lambda x,y:x+y, [negs[x] for x in test_pts])\n",
    "    \n",
    "    files_pos_test = [x for x in files_pos_test if x in admitted_pos]\n",
    "    random.shuffle(files_neg_train)\n",
    "    \n",
    "    files_test = files_pos_test + files_neg_test[:len(files_pos_test)]\n",
    "    \n",
    "    \n",
    "    random.shuffle(files_train)\n",
    "    random.shuffle(files_test)\n",
    "    \n",
    "    return files_train, files_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_map(image, dims_tr = (44, 36)):\n",
    "    h, w = image.shape\n",
    "    \n",
    "    image_tr = np.zeros(dims_tr)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if image[i,j] > 0:\n",
    "                image_tr[int((i / h) // (1.0 / dims_tr[0])),\n",
    "                     int((j / w) // (1.0 / dims_tr[1]))] = 1.0\n",
    "            \n",
    "    return image_tr\n",
    "\n",
    "class DataGenerator_FS(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=16, dim=(170, 140), n_channels=3,\n",
    "                  shuffle=True, OUT_SIZE = (44, 36), TOP_K = 1, mode = 'train'):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.dim = dim # ?-1\n",
    "        self.OUT_SIZE = OUT_SIZE\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.TOP_K = TOP_K\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        self.aug = ImageDataGenerator(rotation_range=45, width_shift_range=0.1,\\\n",
    "                                      height_shift_range=0.1, horizontal_flip=True)\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.OUT_SIZE), dtype='float32')\n",
    "        \n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            \n",
    "            with open('BRATS_preprocessed_/' + ID, 'rb') as filehandler:\n",
    "                photo = pickle.load(filehandler)\n",
    "                \n",
    "            img = photo.image\n",
    "            roi = photo.roi + 1.0\n",
    "            \n",
    "            XX = np.repeat(img.reshape(*self.dim, 1), 3, axis = 2)   \n",
    "            yy = np.repeat(roi.reshape(*self.dim, 1), 3, axis = 2)\n",
    "            \n",
    "            if self.mode == 'train':\n",
    "                params = self.aug.get_random_transform(XX.shape)\n",
    "                X[i,] = self.aug.apply_transform(XX, params)\n",
    "                \n",
    "                \n",
    "                if photo.mal: \n",
    "                    ROI_tr = self.aug.apply_transform(yy, params)\n",
    "                    y[i,:] = downsample_map(ROI_tr[:,:,0], self.OUT_SIZE)\n",
    "                else:\n",
    "                    y[i,:] = np.zeros(self.OUT_SIZE)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                X[i,] = XX\n",
    "                \n",
    "                if photo.mal: \n",
    "                    y[i,:] = downsample_map(yy[:,:, 0], self.OUT_SIZE)\n",
    "                else:\n",
    "                    y[i,:] = np.zeros(self.OUT_SIZE)\n",
    "            \n",
    "              \n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_WS(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=16, dim=(170, 140), n_channels=3,\n",
    "                  shuffle=True, OUT_SIZE = 12, TOP_K = 4, mode = 'train'):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.dim = dim # ?-1\n",
    "        self.OUT_SIZE = OUT_SIZE\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.TOP_K = TOP_K\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        self.aug = ImageDataGenerator(rotation_range=45, width_shift_range=0.1,\\\n",
    "                                      height_shift_range=0.1, horizontal_flip=True)\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.OUT_SIZE), dtype='float32')\n",
    "        \n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            \n",
    "            with open('BRATS_preprocessed_/' + ID, 'rb') as filehandler:\n",
    "                photo = pickle.load(filehandler)\n",
    "                \n",
    "            img = photo.image\n",
    "            \n",
    "            XX = np.repeat(img.reshape(*self.dim, 1), 3, axis = 2)   \n",
    "            \n",
    "            if self.mode == 'train':\n",
    "                params = self.aug.get_random_transform(XX.shape)\n",
    "                X[i,] = self.aug.apply_transform(XX, params)\n",
    "            else:\n",
    "                X[i,] = XX\n",
    "            \n",
    "            yy  = np.zeros(self.OUT_SIZE, dtype = 'float32')\n",
    "            \n",
    "            if photo.mal:\n",
    "                #y[i,:] = np.array([1.0, 0.0])\n",
    "                yy[:self.TOP_K] = 1.0\n",
    "                y[i,:] = yy\n",
    "            else:\n",
    "                #y[i,:] = np.array([0.0, 1.0]) \n",
    "                y[i,:] = yy\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "class My_mul(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super(My_mul, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(My_mul, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.tensordot(x, self.kernel, axes = [[-1],[0]])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        x,y,z,m = input_shape\n",
    "        return (x,y,z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling1 = UpSampling2D(size=(2, 2), data_format=\"channels_last\", interpolation='bilinear')\n",
    "mlp = My_mul()(c_18)\n",
    "print(mlp)\n",
    "upsampling1(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_18 = model.get_layer(\"res3d_branch2c\").output\n",
    "c_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def custom_acc_mil(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.max(K.round(y_pred), axis = -1), K.max(y_true, axis = -1)))\n",
    "\n",
    "def custom_loss_mil(y_true, y_pred):\n",
    "\n",
    "    c1 = tf.clip_by_value(y_pred, 1e-15, 1.0 - 1e-15)\n",
    "    v1 = tf.math.multiply(1.0 - y_true, tf.math.log(1.0 - c1))\n",
    "    v2 = tf.math.multiply(y_true, tf.math.log(c1))\n",
    "\n",
    "    return -tf.reduce_mean(tf.add(v1, v2))#keras.losses.binary_crossentropy\n",
    "\n",
    "\n",
    "def custom_acc_fs(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_pred), y_true))\n",
    "\n",
    "def custom_loss_fs(y_true, y_pred):\n",
    "\n",
    "    c1 = tf.clip_by_value(y_pred, 1e-15, 1.0 - 1e-15)\n",
    "    v1 = tf.math.multiply(1.0 - y_true, tf.math.log(1.0 - c1))\n",
    "    v2 = tf.math.multiply(y_true, tf.math.log(c1))\n",
    "\n",
    "    return -tf.reduce_mean(tf.add(v1, v2))#keras.losses.binary_crossentropy\n",
    "\n",
    "\n",
    "def create_model(mm_, lambd_reg = 5e-6, learning_rate = 5e-4, mu_reg = 1e-3):\n",
    "    \n",
    "    #with tf.variable_scope(\"weights\", reuse=tf.AUTO_REUSE):\n",
    "    #    w1 = tf.get_variable(\"w1\", shape=(256,1), trainable=True)\n",
    "    #    w2 = tf.get_variable(\"w2\", shape=(512,1), trainable=True)\n",
    "    #    w3 = tf.get_variable(\"w3\", shape=(1024,1), trainable=True)\n",
    "    #    w4 = tf.get_variable(\"w4\", shape=(32,1), trainable=True)\n",
    "    \n",
    "    \n",
    "    inp = mm_.input\n",
    "    \n",
    "    c_6 = mm_.get_layer(\"res2c_branch2c\")\n",
    "    #c_6.activity_regularizer = regularizers.l1(lambd_reg)\n",
    "    c_6 = ZeroPadding2D(padding=((1,0),(1,0)))(c_6.output)\n",
    "    \n",
    "    \n",
    "    c_18 = mm_.get_layer(\"res3d_branch2c\")\n",
    "    #c_18.activity_regularizer = regularizers.l1(lambd_reg)\n",
    "    c_18 = c_18.output\n",
    "    \n",
    "    c_42 = mm_.get_layer(\"res4f_branch2c\")\n",
    "    #c_42.activity_regularizer = regularizers.l1(lambd_reg)\n",
    "    c_42 = c_42.output\n",
    "    \n",
    "    '''\n",
    "    c_58 = mm_.get_layer(\"activation_49\")\n",
    "    #c_58.activity_regularizer = regularizers.l1(lambd_reg)\n",
    "    c_58 = c_58.output\n",
    "    '''\n",
    "    \n",
    "    def multy(a):\n",
    "        return lambda x: tf.tensordot(x, a, axes = [[-1],[0]])\n",
    "    \n",
    "    def add(a):\n",
    "        return tf.math.add(a[0], a[1])\n",
    "    \n",
    "    upsampling1 = UpSampling2D(size=(2, 2), data_format=\"channels_last\", interpolation='bilinear')\n",
    "    upsampling2 = UpSampling2D(size=(2, 2), data_format=\"channels_last\", interpolation='bilinear')\n",
    "    upsampling3 = UpSampling2D(size=(2, 2), data_format=\"channels_last\", interpolation='bilinear')\n",
    "    \n",
    "    #layer_4_ = Lambda(multy(w3))(c_42)\n",
    "    layer_4 = My_mul()(c_42)\n",
    "    #print(layer_4, layer_4_)\n",
    "    #print(layer_4.shape, layer_4_.shape)\n",
    "    #print(upsampling1(layer_4).shape, upsampling2(layer_4_).shape)\n",
    "    \n",
    "    #layer_3_ = Lambda(add)([upsampling1(layer_4), Lambda(multy(w2))(c_18)])\n",
    "    \n",
    "    layer_3 = Lambda(add)([upsampling1(layer_4), My_mul()(c_18)])\n",
    "    #print(layer_3.shape, layer_3_.shape)\n",
    "    #layer_2 = Lambda(add)([upsampling2(layer_3), Lambda(multy(w1))(c_6)])\n",
    "    layer_2 = Lambda(add)([upsampling2(layer_3), My_mul()(c_6)])\n",
    "    \n",
    "    conv = Conv2D(1, (1,1), name = 'Conv_Sigmoid', \n",
    "    #              activity_regularizer = regularizers.l1(mu_reg),\\\n",
    "                  activation = 'sigmoid')(layer_2)\n",
    "    \n",
    "    #MIL#######################################\n",
    "    pooled = MaxPooling2D(pool_size = (10,10),\\\n",
    "                              strides = 10, padding='valid')(conv)\n",
    "  \n",
    "    #pooled = MaxPooling2D(pool_size = (44,36),\\\n",
    "    #                          strides = 1, padding='valid')(conv)\n",
    "  \n",
    "    \n",
    "    lda_mil = Lambda(lambda x: tf.contrib.framework.sort(Flatten()(x),\\\n",
    "        axis=-1, direction = 'DESCENDING'), name=\"Sort\")\n",
    "    \n",
    "    #pool_max = MaxPooling2D(pool_size = (4,3),\\\n",
    "    #                          strides = 1, padding='valid')\n",
    "    #out = Dense(2, activation = 'softmax')(lda(pooled))\n",
    "    \n",
    "    #print(pooled)\n",
    "    out_mil = lda_mil(pooled) #pooled\n",
    "    \n",
    "    model_train_mil = Model(inputs = inp, outputs = out_mil)# out_mil\n",
    "    ###################################################\n",
    "    #FS###############################################\n",
    "    lda_fs = Lambda(lambda x: K.squeeze(x, axis = -1))(conv)\n",
    "    model_train_fs = Model(inputs = inp, outputs = lda_fs)\n",
    "    \n",
    "    for layer in model_train_mil.layers:\n",
    "        layer.kernel_regularizer = regularizers.l2(lambd_reg)\n",
    "        \n",
    "    model_image = Model(inputs = inp, outputs = conv)\n",
    "    \n",
    "    adam_mil = Adam(learning_rate / 100.0)#SGD(lr = learning_rate / 100.0, momentum = 0.9, nesterov = True)#Adam(learning_rate / 100.0)\n",
    "    adam_fs = Adam(learning_rate)#SGD(lr = learning_rate, momentum = 0.9, nesterov = True)#Adam(learning_rate)\n",
    "    \n",
    "    model_train_mil.compile(optimizer = adam_mil, loss = custom_loss_mil, metrics = [custom_acc_mil])\n",
    "    model_train_fs.compile(optimizer = adam_fs, loss = dice_coef_loss, metrics = [custom_acc_fs, dice_coef_loss])\n",
    "    \n",
    "    #model_train.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model_train_mil, model_train_fs, model_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patients_pos', 'rb') as filehandler:\n",
    "    patients_pos = pickle.load(filehandler)\n",
    "    \n",
    "with open('patients_neg', 'rb') as filehandler:\n",
    "    patients_neg = pickle.load(filehandler)\n",
    "    \n",
    "with open('10_percent_lesions', 'rb') as filehandler:\n",
    "    admitted_pos = pickle.load(filehandler)   \n",
    "    \n",
    "train_namespace, val_namespace = create_train_test_sets(\\\n",
    "                patients_pos, patients_neg, admitted_pos)\n",
    "\n",
    "N_fs = 100\n",
    "\n",
    "train_namespace_fs = train_namespace[:N_fs]\n",
    "train_namespace_mil = train_namespace[N_fs:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(['Brats18_TCIA01_448_1' in x for x in train_namespace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(val_namespace).intersection(set(train_namespace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False, weights='imagenet', input_shape = (170, 140, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in mm_fs.get_weights():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_mil, mm_fs, mm_image = create_model(model)\n",
    "\n",
    "training_generator_mil = DataGenerator_WS(train_namespace_mil, mode = 'train')\n",
    "training_generator_fs = DataGenerator_FS(train_namespace_fs, mode = 'train')\n",
    "\n",
    "\n",
    "validation_generator_mil = DataGenerator_WS(val_namespace, mode = 'val')\n",
    "validation_generator_fs = DataGenerator_FS(val_namespace, mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_fs.save_weights('coeffs_FS/three.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"coeffs_FS/weights-improvement-{epoch:02d}-{val_dice_coef_loss:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_dice_coef_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#callbacks_list = [checkpoint]\n",
    "\n",
    "N_epochs = 100\n",
    "current_dice = 1.0\n",
    "\n",
    "tr_acc = []\n",
    "tr_loss = []\n",
    "tr_dice = []\n",
    "\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "val_dice = []\n",
    "\n",
    "#mm_fs.load_weights('coeffs_FS/epoch_37_dice_0.220_.hdf5')\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    print(\"Epoch: %d \\n\\n\"%(epoch))\n",
    "    history = mm_fs.fit_generator(generator=training_generator_fs,\n",
    "            validation_data=validation_generator_fs,\n",
    "            use_multiprocessing=True,\n",
    "             #callbacks=callbacks_list,\n",
    "            workers=4, verbose = 1, epochs = 1)\n",
    "    \n",
    "    hist = history.history\n",
    "    \n",
    "    tr_acc.append(hist['custom_acc_fs'][0])\n",
    "    tr_loss.append(hist['loss'][0])\n",
    "    tr_dice.append(hist['dice_coef_loss'][0])\n",
    "    \n",
    "    val_acc.append(hist['val_custom_acc_fs'][0])\n",
    "    val_loss.append(hist['val_loss'][0])\n",
    "    val_dice.append(hist['val_dice_coef_loss'][0])\n",
    "\n",
    "    if val_dice[-1] < current_dice:\n",
    "        current_dice = val_dice[-1]\n",
    "        mm_fs.save_weights('coeffs_FS_1/epoch_' + str(epoch) + '_dice_'+ \"%.3f\" % current_dice +'_.hdf5')\n",
    "        \n",
    "#plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, N_epochs + 1), tr_acc, label = 'Accuracy_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_acc, label = 'Accuracy_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, N_epochs + 1), tr_loss, label = 'Loss_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_loss, label = 'Loss_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, N_epochs + 1), tr_dice, label = 'Dice_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_dice, label = 'Dice_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Dice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 100\n",
    "plt.plot(range(1, N_epochs + 1), tr_acc, label = 'Accuracy_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_acc, label = 'Accuracy_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, N_epochs + 1), tr_loss, label = 'Loss_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_loss, label = 'Loss_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, N_epochs + 1), tr_dice, label = 'Dice_train')\n",
    "plt.plot(range(1, N_epochs + 1), val_dice, label = 'Dice_val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Dice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_fs.load_weights('coeffs_FS_1/epoch_0_dice_0.236_.hdf5')\n",
    "#mm_mil.optimizer = Adam()\n",
    "#K.set_value(mm_mil.optimizer.lr, 5e-7)\n",
    "\n",
    "print(mm_fs.evaluate_generator(generator=validation_generator_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 20\n",
    "\n",
    "mm_fs.fit_generator(generator=training_generator_fs,\n",
    "        validation_data=validation_generator_fs,\n",
    "        use_multiprocessing=True,\n",
    "         #callbacks=callbacks_list,\n",
    "        workers=4, verbose = 1, epochs = 2)\n",
    "\n",
    "\n",
    "print('!!!!!\\n\\n\\n\\n\\n')\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    print(\"Epoch #%d MIL\\n\"%(epoch))\n",
    "\n",
    "    mm_mil.fit_generator(generator=training_generator_mil,\n",
    "                validation_data=validation_generator_mil,\n",
    "                use_multiprocessing=True,\n",
    "                 #callbacks=callbacks_list,\n",
    "                workers=4, verbose = 1, epochs = 1)\n",
    "    \n",
    "    print(mm_fs.evaluate_generator(generator=validation_generator_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 20\n",
    "\n",
    "print('!!!!!\\n\\n\\n\\n\\n')\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    print(\"Epoch #%d MIL\\n\"%(epoch))\n",
    "\n",
    "    mm_mil.fit_generator(generator=training_generator_mil,\n",
    "                validation_data=validation_generator_mil,\n",
    "                use_multiprocessing=True,\n",
    "                 #callbacks=callbacks_list,\n",
    "                workers=4, verbose = 1, epochs = 1)\n",
    "    \n",
    "    print(mm_fs.evaluate_generator(generator=validation_generator_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 20\n",
    "\n",
    "mm_fs.fit_generator(generator=training_generator_fs,\n",
    "        validation_data=validation_generator_fs,\n",
    "        use_multiprocessing=True,\n",
    "         #callbacks=callbacks_list,\n",
    "        workers=4, verbose = 1, epochs = 2)\n",
    "\n",
    "K.set_value(mm_fs.optimizer.lr, 5e-7)\n",
    "\n",
    "\n",
    "print('!!!!!\\n\\n\\n\\n\\n')\n",
    "for epoch in range(N_epochs):\n",
    "    \n",
    "    print(\"Epoch #%d MIL\\n\"%(epoch))\n",
    "    \n",
    "    mm_mil.fit_generator(generator=training_generator_mil,\n",
    "                validation_data=validation_generator_mil,\n",
    "                use_multiprocessing=True,\n",
    "                 #callbacks=callbacks_list,\n",
    "                workers=4, verbose = 1, epochs = 1)\n",
    "    \n",
    "    print(\"Epoch #%d FS\\n\"%(epoch))\n",
    "    \n",
    "    mm_fs.fit_generator(generator=training_generator_fs,\n",
    "        validation_data=validation_generator_fs,\n",
    "        use_multiprocessing=True,\n",
    "         #callbacks=callbacks_list,\n",
    "        workers=4, verbose = 1, epochs = 1)\n",
    "    \n",
    "    print(mm_fs.evaluate_generator(generator=validation_generator_fs))\n",
    "    \n",
    "    K.set_value(mm_fs.optimizer.lr, K.get_value(mm_fs.optimizer.lr) * 0.9)\n",
    "    K.set_value(mm_mil.optimizer.lr, K.get_value(mm_mil.optimizer.lr) * 0.9)\n",
    "    \n",
    "    print(mm_fs.evaluate_generator(generator=validation_generator_fs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm_fs.evaluate_generator(generator=validation_generator_fs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_fs.fit_generator(generator=training_generator_fs,\n",
    "        validation_data=validation_generator_fs,\n",
    "        use_multiprocessing=True,\n",
    "         #callbacks=callbacks_list,\n",
    "        workers=4, verbose = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from skimage.transform import rescale, resize\n",
    "for ID in val_namespace:\n",
    "#ID = namespace[400]\n",
    "\n",
    "\n",
    "    with open('BRATS_preprocessed_/' + ID, 'rb') as filehandler:\n",
    "        photo = pickle.load(filehandler)\n",
    "\n",
    "    print(photo.mal)\n",
    "    #with open('Data_original/Original/' + ID, 'rb') as filehandler:\n",
    "    #    photo = pickle.load(filehandler, encoding = 'latin1')\n",
    "    \n",
    "    mm_fs.load_weights('coeffs_FS_1/epoch_0_dice_0.236_.hdf5')\n",
    "    pred1 = mm_image.predict(np.repeat(photo.image.reshape(1, 170, 140, 1), 3, axis=3))\n",
    "    \n",
    "    #mm_fs.load_weights('coeffs_FS/two.hdf5')\n",
    "    pred2 = mm_image.predict(np.repeat(photo.image.reshape(1, 170, 140, 1), 3, axis=3))\n",
    "    \n",
    "    #mm_fs.load_weights('coeffs_FS_1/epoch_93_dice_0.121_.hdf5')\n",
    "    pred3 = mm_image.predict(np.repeat(photo.image.reshape(1, 170, 140, 1), 3, axis=3))\n",
    "    \n",
    "    \n",
    "    #mm_fs.load_weights('coeffs_FS/one.hdf5')\n",
    "    pred4 = mm_image.predict(np.repeat(photo.image.reshape(1, 170, 140, 1), 3, axis=3))\n",
    "    \n",
    "    plt.figure(figsize=(25,50))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    photo.print_image()\n",
    "    photo.print_roi()\n",
    "    plt.title('Original image')\n",
    "\n",
    "    #plt.show()\n",
    "    \n",
    "    photo.image = resize(photo.image, (44,36),\n",
    "                       anti_aliasing=True)\n",
    "    \n",
    "    photo.roi = resize(photo.roi, (44,36),\n",
    "                       anti_aliasing=True)\n",
    "    plt.subplot(1, 6, 2)\n",
    "    photo.print_roi()\n",
    "    plt.imshow(pred1.reshape((44,36)), cmap = 'Blues')\n",
    "    #plt.colorbar()\n",
    "    #plt.clim(0, 1)\n",
    "    plt.title(\"FS_30_epochs_dice_0.223\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 6, 3)\n",
    "    photo.print_roi()\n",
    "    plt.imshow(pred2.reshape((44,36)), cmap = 'Blues')\n",
    "    #plt.colorbar()\n",
    "    #plt.clim(0, 1)\n",
    "    plt.title(\"two\")\n",
    "    \n",
    "    plt.subplot(1, 6, 4)\n",
    "    photo.print_roi()\n",
    "    plt.imshow(pred3.reshape((44,36)), cmap = 'Blues')\n",
    "    #plt.colorbar()\n",
    "    #plt.clim(0, 1)\n",
    "    plt.title('three')\n",
    "    #plt.title(\"FS_66_epochs_dice_0.129\")\n",
    "    \n",
    "    plt.subplot(1, 6, 5)\n",
    "    photo.print_roi()\n",
    "    plt.imshow(pred4.reshape((44,36)), cmap = 'Blues')\n",
    "    #plt.colorbar()\n",
    "    #plt.clim(0, 1)\n",
    "    plt.title('MIL_dice_0.114')\n",
    "    \n",
    "    plt.subplot(1, 6, 6)\n",
    "    photo.print_roi()\n",
    "    plt.imshow(pred4.reshape((44,36)), cmap = 'Blues')\n",
    "    #plt.colorbar()\n",
    "    #plt.clim(0, 1)\n",
    "    plt.title('MIL_dice_0.114')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 1))\n",
    "    fig.subplots_adjust(bottom=0.6, right = 2.4)\n",
    "\n",
    "    cmap = 'Blues'#mpl.cm.cool\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    cb1 = mpl.colorbar.ColorbarBase(ax, cmap=cmap,\n",
    "                                    norm=norm,\n",
    "                                    orientation='horizontal')\n",
    "    cb1.set_label('Probability')\n",
    "    fig.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_mil.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_fs.evaluate_generator(generator=training_generator_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_fs.optimizer.lr.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "K.set_value(mm_fs.optimizer.lr, 1e-3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(mm_fs.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.get_value(mm_fs.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
